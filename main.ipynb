{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import seaborn\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing \n",
    "from sklearn.linear_model import LogisticRegression  #l逻辑回归\n",
    "from sklearn.ensemble import RandomForestClassifier   #随机森林\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn import metrics \n",
    "import scikitplot \n",
    "#Hide all the warnings in jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 解压数据\n",
    "!unzip /home/aistudio/data/data190996/MachineLearningCSV.zip -d ./data58344/\n",
    "!unzip /home/aistudio/data/data190996/GeneratedLabelledFlows.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "'''\n",
    "data0 = pd.read_csv('data58344/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',  low_memory = False)\n",
    "data1 = pd.read_csv('data58344/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', low_memory = False)\n",
    "data2 = pd.read_csv('data58344/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv',  low_memory = False)\n",
    "data3 = pd.read_csv('data58344/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv',  low_memory = False)\n",
    "data4 = pd.read_csv('data58344/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', low_memory = False)\n",
    "data5 = pd.read_csv('data58344/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',  low_memory = False)\n",
    "data6 = pd.read_csv('data58344/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv', low_memory = False)\n",
    "data7 = pd.read_csv('data58344/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv', low_memory = False)\n",
    "data_total = pd.concat([data0,data1,data2,data3,data4,data5,data6,data7])\n",
    "data_total.to_csv(\"X_combine.csv\",index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#n = 100000  # 随机选择10行\n",
    "filename = \"X_combine.csv\"  # 文件名\n",
    "\n",
    "data_total = pd.read_csv(filename)\n",
    "#data_total = data_total.sample(n=n)\n",
    "print(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#统计缺失值数据\n",
    "\n",
    "data_total.isnull().sum()\n",
    "\n",
    "data_total = data_total.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 无穷值或者空值处理  空值替换INF 最后删除\n",
    "data_total = data_total.replace([np.inf, -np.inf], np.nan).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_total.to_csv(\"Merge_2017_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 检查每列的值是否相同\n",
    "to_drop = []\n",
    "for column in data_total.columns:\n",
    "    if data_total[column].nunique() == 1:\n",
    "        to_drop.append(column)\n",
    "\n",
    "# 删除需要删除的列\n",
    "data_total.drop(columns=to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**划分数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看数据结构，划分数据\n",
    "print(data_total.shape)\n",
    "df=data_total\n",
    "X = df.loc[:,df.columns != \" Label\"]\n",
    "Y = df.loc[:,df.columns == \" Label\"]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 查看数据分布\n",
    "def ClassDistribution(y):\n",
    "    ax = sns.countplot(y[' Label'],label=\"Count\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    print(y.groupby(' Label').size())\n",
    "\n",
    "ClassDistribution(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 无穷值或者空值处理\n",
    "import numpy as np\n",
    "X.fillna(0, inplace=True)\n",
    "train_X = np.isinf(X)\n",
    "X[train_X]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, train_size = 0.8, test_size = 0.2, random_state = 42) #shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 调用模型进行训练，换其他模型改这里就行\n",
    "xg = xgb.XGBClassifier(n_estimators = 10)\n",
    "xg.fit(X_train,y_train)\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "y_predict=xg.predict(X_test)\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "print(classification_report(y_true,y_predict))\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下是降维方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.0 SelectKBest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#选择出前k个与标签最相关的特征。\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "selector = SelectKBest(f_classif, k=30)\n",
    "selector.fit(X, Y)\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.0  互信息分类**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "n = 5000  # 随机选择10行\n",
    "filename = \"X_combine.csv\"  # 文件名\n",
    "\n",
    "data_sample = pd.read_csv(filename)\n",
    "data_sample = data_sample.sample(n=n)\n",
    "print(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#统计缺失值数据\n",
    "data_sample.isnull().sum()\n",
    "data_sample = data_sample.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 检查每列的值是否相同\n",
    "to_drop_two= []\n",
    "for column in data_sample.columns:\n",
    "    if data_sample[column].nunique() == 1:\n",
    "        to_drop_two.append(column)\n",
    "\n",
    "# 删除需要删除的列\n",
    "data_sample.drop(columns=to_drop_two, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#更新后的数据维度\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sample[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 查看数据结构，划分数据\n",
    "df_two=data_sample\n",
    "X_two = df_two.loc[:,df_two.columns != \" Label\"]\n",
    "Y_two = df_two.loc[:,df_two.columns == \" Label\"]\n",
    "print(X_two.shape)\n",
    "print(Y_two.shape)\n",
    "# 无穷值或者空值处理\n",
    "import numpy as np\n",
    "X_two.fillna(0, inplace=True)\n",
    "train_X_two = np.isinf(X_two)\n",
    "X_two[train_X_two]=0\n",
    "X_two.shape\n",
    "Y_two.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_two.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_two[' Label'] = le.fit_transform(df_two[' Label']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#统计\n",
    "df_two[' Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances = mutual_info_classif(X_two, Y_two)#互信息分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Min-max normalization\n",
    "numeric_features = df_two.dtypes[df_two.dtypes != 'object'].index\n",
    "df_two[numeric_features] = df_two[numeric_features].apply(\n",
    "    lambda x: (x - x.min()) / (x.max()-x.min()))\n",
    "# Fill empty values by 0\n",
    "df_two = df_two.fillna(0)\n",
    "\n",
    "'''\n",
    "# Z-score normalization\n",
    "numeric_features = df_two.dtypes[df_two.dtypes != 'object'].index\n",
    "df_two[numeric_features] = df_two[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# Fill empty values by 0\n",
    "df_two = df_two.fillna(0)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the sum of importance scores\n",
    "#  计算重要性分数的总和\n",
    "f_list = sorted(zip(map(lambda x: round(x, 4), importances), numeric_features), reverse=True)\n",
    "Sum = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list)):\n",
    "    Sum = Sum + f_list[i][0]\n",
    "    fs.append(f_list[i][1])\n",
    "    \n",
    "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "# 从上到下选择重要特征，直到累积重要性达到90%\n",
    "f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), numeric_features), reverse=True)\n",
    "Sum2 = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list2)):\n",
    "    Sum2 = Sum2 + f_list2[i][0]\n",
    "    fs.append(f_list2[i][1])\n",
    "    if Sum2>=0.90:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fs = df_two[fs]\n",
    "X_fs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(X_fs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_three = df_three.drop([' Label'],axis=1)\n",
    "#Y_three = df_three[' Label']\n",
    "#X_train_three, X_test_three, Y_train_three, Y_test_three = train_test_split(X_three,Y_three, train_size = 0.8, test_size = 0.2, random_state = 42) #shuffle=False\n",
    "X_two = df_two.loc[:,df_two.columns != \" Label\"]\n",
    "Y_two = df_two.loc[:,df_two.columns == \" Label\"]\n",
    "X_train_two, X_test_two, Y_train_two, Y_test_two = train_test_split(X_two,Y_two, train_size = 0.8, test_size = 0.2, random_state = 42) #shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train_two.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#调用随机森林模型训练\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 定义随机森林模型\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=123)\n",
    "# 在训练集上拟合模型\n",
    "train_model_rf = rf_model.fit(X_two, Y_two)\n",
    "# 计算准确率\n",
    "print(\"Score:\", rf_model.score(X_two,Y_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "df_three=copy.deepcopy(df_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_three = df_three.drop([' Label'],axis=1)\n",
    "Y_three = df_three[' Label']\n",
    "X_train_three, X_test_three, Y_train_three, Y_test_three = train_test_split(X_three,Y_three, train_size = 0.8, test_size = 0.2, random_state = 42) #shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#object编码  train['labels']\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoding = LabelEncoder()\n",
    "train['labels'] = label_encoding.fit_transform()\n",
    "train['protocol_type'] = label_encoding.fit_transform(train['protocol_type'])\n",
    "train['service'] = label_encoding.fit_transform(train['service'])\n",
    "train['flag'] = label_encoding.fit_transform(train['flag'])\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_three[' Label'] = le.fit_transform(df_three[' Label']).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#编码方式已经改变\n",
    "df_three[' Label'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_three[' Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SMOTE解决类不平衡\n",
    "pd.Series(df_three[' Label']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(n_jobs=-1,sampling_strategy={1:200,10:200,11:200,7:10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_three = df_three.drop([' Label'],axis=1)\n",
    "Y_three = df_three[' Label']\n",
    "X_train_three, X_test_three, Y_train_three, Y_test_three = train_test_split(X_three,Y_three, train_size = 0.8, test_size = 0.2, random_state = 42) #shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train_three, Y_train_three = smote.fit_resample(X_train_three, Y_train_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 热力分析（降维用的，具体不了解）\n",
    "f,ax = plt.subplots(figsize=(24, 24))\n",
    "corr = X.corr()\n",
    "sns.heatmap(corr, annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
    "plt.savefig('heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 降维\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "to_drop = []\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.75:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "                to_drop.append((corr.columns)[j])\n",
    "print('删除高度相关的特征：', to_drop)\n",
    "X = X.drop(to_drop,axis = 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 降维\n",
    "def RemoveMissing(train):\n",
    "    train_missing = (train.isnull().sum() / len(train)).sort_values(ascending = False)\n",
    "    #train_missing.head()\n",
    "    train_missing = train_missing.index[train_missing > 0.75]\n",
    "    all_missing = list(set(train_missing))\n",
    "    train.drop(all_missing,axis = 1)\n",
    "    print('There are %d columns with more than 75%% missing values' % len(all_missing))\n",
    "    return train\n",
    "X = RemoveMissing(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove columns with only 1 unique value\n",
    "# 如果某特征只有一个值，删除该特征\n",
    "def remove_single_unique_values(dataframe):\n",
    "    # 得到每一列唯一数值的数量\n",
    "    cols_to_drop = dataframe.nunique()\n",
    "    print(cols_to_drop)\n",
    "    # 得到每一列唯一数值为1个的列标签\n",
    "    cols_to_drop = cols_to_drop.loc[cols_to_drop.values==1].index\n",
    "    print(cols_to_drop)\n",
    "    print('There are %d columns with only 1 unique value' % len(cols_to_drop))\n",
    "    # 删除唯一数值的特征\n",
    "    dataframe = dataframe.drop(cols_to_drop,axis=1)\n",
    "    return dataframe\n",
    "\n",
    "X = remove_single_unique_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 保存一下处理后的数据\n",
    "X.to_csv('HandleData_X_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = \"HandleData_X_processed.csv\"  # 文件名\n",
    "\n",
    "X = pd.read_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.info())\n",
    "print(Y[\" Label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Y[' Label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对Label进行编码\n",
    "Y[' Label'].replace(to_replace=['BENIGN',\n",
    "                'PortScan',\n",
    "                'DDoS',\n",
    "                'DoS slowloris',\n",
    "                'DoS Slowhttptest',\n",
    "                'DoS Hulk',\n",
    "                'DoS GoldenEye',\n",
    "                'Heartbleed',\n",
    "                'FTP-Patator',\n",
    "                'SSH-Patator',\n",
    "                'Web Attack � Brute Force',\n",
    "                'Web Attack � XSS',\n",
    "                'Web Attack � Sql Injection',\n",
    "                'Infiltration',\n",
    "                'Bot'], \n",
    "            value=[ 0\n",
    "                    ,1\n",
    "                    ,2\n",
    "                    ,3\n",
    "                    ,4\n",
    "                    ,5\n",
    "                    ,6\n",
    "                    ,7\n",
    "                    ,8\n",
    "                    ,9\n",
    "                    ,10\n",
    "                    ,11\n",
    "                    ,12\n",
    "                    ,13\n",
    "                    ,14],\n",
    "            inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Y[' Label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 无穷值或者空值处理\n",
    "X.fillna(0, inplace=True)\n",
    "train_X = np.isinf(X)\n",
    "X[train_X]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 调用模型进行训练，换其他模型改这里就行\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=12, min_samples_leaf=1, splitter=\"best\")\n",
    "trained_model = clf.fit(X,Y)\n",
    "print(\"Score:\", trained_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#调用SVM模型训练\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "# 建立模型\n",
    "model_svc = SVC(kernel='linear')\n",
    "#训练模型\n",
    "train_model_svc = model_svc.fit(X,Y)\n",
    "# 计算准确率\n",
    "print(\"Score:\", train_model_svc.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#调用随机森林模型训练\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 定义随机森林模型\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=123)\n",
    "# 在训练集上拟合模型\n",
    "train_model_rf = rf_model.fit(X, Y)\n",
    "# 计算准确率\n",
    "print(\"Score:\", rf_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#调用KNN模型训练\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 创建一个 KNN 分类器对象\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "# 在训练集上拟合模型\n",
    "train_model_knn = knn_model.fit(X, Y)\n",
    "# 计算准确率\n",
    "print(\"Score:\", knn_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#神经网络\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 模型保存\n",
    "import pickle\n",
    "with open('clf.pickle','wb')as f: \n",
    "    pickle.dump(clf,f)\n",
    "# 模型加载\n",
    "with open('clf.pickle', 'rb') as f:\n",
    "   clf2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 随机森林模型保存\n",
    "import pickle\n",
    "with open('rf.pickle','wb')as f: \n",
    "    pickle.dump(rf_model,f)\n",
    "# 模型加载\n",
    "with open('rf.pickle', 'rb') as f:\n",
    "   rf_model2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# knn模型保存\n",
    "import pickle\n",
    "with open('knn.pickle','wb')as f: \n",
    "    pickle.dump(knn_model,f)\n",
    "# 模型加载\n",
    "with open('knn.pickle', 'rb') as f:\n",
    "   knn_model2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取待预测数据\n",
    "mydata0 = pd.read_csv('Mydata/202211280951-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata2 = pd.read_csv('Mydata/20221128-10.19.2.20-0-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata3 = pd.read_csv('Mydata/20221128-10.19.2.20-1-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata4 = pd.read_csv('Mydata/20221128-10.19.2.20-2-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata5 = pd.read_csv('Mydata/20221128-10.19.2.20-3-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata6 = pd.read_csv('Mydata/20221128-10.19.2.20-4-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata7 = pd.read_csv('Mydata/20221128-10.19.3.44-0-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata8 = pd.read_csv('Mydata/20221128-10.19.3.44-1-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata9 = pd.read_csv('Mydata/20221128-10.19.3.44-2-ens224.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "mydata10 = pd.read_csv('Mydata/202211280943.pcap_Flow.csv', low_memory = False,encoding='gbk')\n",
    "\n",
    "mydata1= pd.concat([mydata0,mydata2,mydata3,mydata4,mydata5,mydata6,mydata7,mydata8,mydata9,mydata10])\n",
    "mydata1.to_csv('Y_combine.csv')\n",
    "#mydata1 = mydata1.rename(columns={' act data pkt fwd':'Fwd Act Data Pkts'})\n",
    "print(mydata1.info())\n",
    "X_test = mydata1.loc[:,mydata1.columns != \" Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练时用到的特征值\n",
    "imps = [' Average Packet Size', \n",
    "        ' Packet Length Mean', \n",
    "        ' Packet Length Std',\n",
    "       ' Packet Length Variance', \n",
    "       ' Total Length of Bwd Packets',\n",
    "       'Total Length of Fwd Packets', \n",
    "       ' Subflow Fwd Bytes',\n",
    "       ' Subflow Bwd Bytes', \n",
    "       ' Bwd Packet Length Mean',\n",
    "       ' Avg Bwd Segment Size',\n",
    "        'Init_Win_bytes_forward',\n",
    "       ' Init_Win_bytes_backward',\n",
    "        'Bwd Packet Length Max',\n",
    "       ' Max Packet Length', \n",
    "       ' Fwd Packet Length Max',\n",
    "        ' Fwd Header Length.1',\n",
    "       ' Fwd Header Length', \n",
    "       ' Flow IAT Max', \n",
    "       ' Destination Port',\n",
    "       ' Flow Duration',\n",
    "        ' Bwd Header Length', \n",
    "        ' Fwd IAT Max',\n",
    "         'Flow Bytes/s',\n",
    "       'Fwd IAT Total', \n",
    "       ' Bwd Packets/s', \n",
    "       'Fwd Packets/s', \n",
    "       ' Flow Packets/s',\n",
    "       ' Fwd IAT Mean', \n",
    "       ' Flow IAT Mean', \n",
    "       ' Fwd Packet Length Mean',\n",
    "       ' Avg Fwd Segment Size', ' Subflow Bwd Packets', ' Flow IAT Std',\n",
    "       ' Bwd Packet Length Std', ' Total Backward Packets', ' Fwd IAT Std',\n",
    "       'Subflow Fwd Packets', ' Total Fwd Packets', 'Bwd IAT Total',\n",
    "       ' Bwd IAT Max', ' Bwd Packet Length Min', ' Fwd Packet Length Std',\n",
    "       ' Bwd IAT Mean', ' min_seg_size_forward', ' Bwd IAT Min',\n",
    "       ' act_data_pkt_fwd', ' Fwd IAT Min', ' Min Packet Length', 'Idle Mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 取出对应特征值的数据并且进行处理\n",
    "X_test = X_test.loc[:, X.columns]\n",
    "test_X = np.isinf(X_test)\n",
    "X_test[test_X]=0\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_pred = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看数据种类\n",
    "ydata1_1 = pd.DataFrame(y_pred)\n",
    "print(ydata1_1[0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_name = ['BENIGN',\n",
    "                'PortScan',\n",
    "                'DDoS',\n",
    "                'DoS slowloris',\n",
    "                'DoS Slowhttptest',\n",
    "                'DoS Hulk',\n",
    "                'DoS GoldenEye',\n",
    "                'Heartbleed',\n",
    "                'FTP-Patator',\n",
    "                'SSH-Patator',\n",
    "                'Web Attack � Brute Force',\n",
    "                'Web Attack � XSS',\n",
    "                'Web Attack � Sql Injection',\n",
    "                'Infiltration',\n",
    "                'Bot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对编码后的标签值进行还原\n",
    "ydata1_0 = pd.DataFrame(ydata1_1)\n",
    "mydata1['Label'] = ydata1_0\n",
    "mydata1['Label'].replace(to_replace=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], value=label_name, inplace=True)\n",
    "print(mydata1['Label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看分布\n",
    "def ClassDistribution(y):\n",
    "    ax = sns.countplot(y[0],label=\"Count\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    print(y.groupby(0).size())\n",
    "\n",
    "ClassDistribution(ydata1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
